{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "from open_spiel.python import rl_environment\n",
    "from acme.wrappers import open_spiel_wrapper\n",
    "import numpy as np\n",
    "import dm_env\n",
    "\n",
    "class AlphaZeroWrapper(open_spiel_wrapper.OpenSpielWrapper):\n",
    "    def __init__(self, environment: rl_environment.Environment, history_size: int = 8):\n",
    "        super().__init__(environment)\n",
    "        self._history_size = history_size\n",
    "        self._board_size = 19\n",
    "        self._state_history = []\n",
    "        self._num_planes = self._history_size * 2 + 1  # 2 planes per history step + 1 for current player\n",
    "\n",
    "    def _convert_obs(self, observations: List[open_spiel_wrapper.OLT]) -> List[open_spiel_wrapper.OLT]:\n",
    "        # Extract the current board state (4 planes: black, white, empty, current player)\n",
    "        # Here each observation in observations is identical(perfect information game), \n",
    "        # please refer to https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/games/go/go.cc#L109\n",
    "        current_state = observations[0].observation.reshape(self._board_size, self._board_size, 4)\n",
    "        \n",
    "        # Update state history (we'll only use the first 2 planes for history)\n",
    "        self._state_history.append(current_state)\n",
    "        if len(self._state_history) > self._history_size:\n",
    "            self._state_history.pop(0)\n",
    "        \n",
    "        # Construct the n-plane representation\n",
    "        alphazero_observation = self._construct_alphazero_planes()\n",
    "        \n",
    "        # Update the observation in the OLT named tuple for both players\n",
    "        new_observations = []\n",
    "        for obs in observations:\n",
    "            new_obs = open_spiel_wrapper.OLT(\n",
    "                observation=alphazero_observation,\n",
    "                legal_actions=obs.legal_actions,\n",
    "                terminal=obs.terminal\n",
    "            )\n",
    "            new_observations.append(new_obs)\n",
    "        \n",
    "        return new_observations\n",
    "\n",
    "    def _construct_alphazero_planes(self):\n",
    "        observation = np.zeros((self._board_size, self._board_size, self._num_planes), dtype=np.float32)\n",
    "        \n",
    "        for i, state in enumerate(reversed(self._state_history)):\n",
    "            if i >= self._history_size:\n",
    "                break\n",
    "            observation[:, :, i*2] = state[:, :, 0]  # Black stones\n",
    "            observation[:, :, i*2+1] = state[:, :, 1]  # White stones\n",
    "        \n",
    "        # Set the current player plane\n",
    "        current_player = self._state_history[-1][:, :, 3]  # Use the 4th plane from the most recent state\n",
    "        observation[:, :, -1] = current_player\n",
    "        \n",
    "        return observation\n",
    "\n",
    "    def _initialize_state_history(self):\n",
    "        empty_state = np.zeros((self._board_size, self._board_size, self._num_planes), dtype=np.float32)\n",
    "        self._state_history = [empty_state] * self._history_size\n",
    "\n",
    "    def observation_spec(self):\n",
    "        spec = super().observation_spec()\n",
    "        new_shape = (self._board_size, self._board_size, self._num_planes)\n",
    "        return open_spiel_wrapper.OLT(\n",
    "            observation=dm_env.specs.BoundedArray(shape=new_shape, dtype=np.float32, name='observation', minimum=0, maximum=1),\n",
    "            legal_actions=spec.legal_actions,\n",
    "            terminal=spec.terminal\n",
    "        )\n",
    "\n",
    "    def reset(self) -> dm_env.TimeStep:\n",
    "        timestep = super().reset()\n",
    "        self._initialize_state_history()\n",
    "        return self._update_timestep(timestep)\n",
    "\n",
    "    def step(self, action: int) -> dm_env.TimeStep:\n",
    "        timestep = super().step([action])\n",
    "        return self._update_timestep(timestep)\n",
    "\n",
    "    def _update_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n",
    "        new_observation = self._convert_obs(timestep.observation)\n",
    "        return dm_env.TimeStep(\n",
    "            step_type=timestep.step_type,\n",
    "            reward=timestep.reward,\n",
    "            discount=timestep.discount,\n",
    "            observation=new_observation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acme import wrappers\n",
    "\n",
    "env_configs = {\n",
    "        'max_game_length': 3,\n",
    "        'komi': 7.5,\n",
    "        'board_size': 19,\n",
    "    }\n",
    "raw_environment = rl_environment.Environment('go', **env_configs)\n",
    "environment = AlphaZeroWrapper(raw_environment)\n",
    "environment = wrappers.SinglePrecisionWrapper(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep = environment.reset()\n",
    "obs = timestep.observation\n",
    "environment.current_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_actions0 = obs[0].legal_actions\n",
    "legal_actions1 = obs[1].legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep1 = environment.step(2)\n",
    "obs1 = timestep1.observation\n",
    "environment.current_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_actions0 = obs1[0].legal_actions\n",
    "legal_actions1 = obs1[1].legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep1.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep2 = environment.step(4)\n",
    "obs2 = timestep2.observation\n",
    "environment.current_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep3 = environment.step(8)\n",
    "obs3 = timestep3.observation\n",
    "environment.current_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep3.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = [0.5] * 362"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = tf.where(legal_actions > 0, policy, tf.float32.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (362,)\n",
      "Values: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "Total actions: DiscreteArray(shape=(), dtype=int32, name=None, minimum=0, maximum=361, num_values=362)\n"
     ]
    }
   ],
   "source": [
    "legal_actions = obs[0].legal_actions\n",
    "print(\"Shape:\", np.shape(legal_actions))\n",
    "print(\"Values:\", legal_actions)\n",
    "print(\"Total actions:\", environment.action_spec())  # Usually board_size * board_size + 1 for pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme-mcts-39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
